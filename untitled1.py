# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DEUu04xb9JjabGl3OWZefPZXSFVmEVxy
"""

#importing required libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

#connecting to the google drive
from google.colab import drive
drive.mount('/content/drive')

#read data file using pandas
data = pd.read_csv("/content/drive/MyDrive/A_1min.csv")
data

print(data.columns) #columns in a dataset

"""# 1.minute by minute plot"""

#1.minute by minute plot for the stocks dataset for a specific year
def plot_closing_vs_minutebyminute(year, data , label_frequency =2000):
    # Converting 'date time' column to datetime format
    data['date time'] = pd.to_datetime(data['date time'])
    # Filter data for the desired year
    data_year = data[data['date time'].dt.year == year]
    # Extract time and closing price
    time = data_year['date time']
    closing_price = data_year['close']
    # Plotting
    plt.figure(figsize =(14,5))
    plt.plot(time,closing_price)
    plt.title(f'Closing Prices vs. Time for {year}')
    plt.xlabel('Time')
    plt.ylabel('Close Price')
    plt.xticks(time[::label_frequency], time.dt.strftime('%H:%M:%S')[::label_frequency], rotation=45)
    plt.show()

plot_closing_vs_minutebyminute(2018,data)

"""#1.2 day by day plot"""

#plot day by day closing price stocks
def plot_closing_day_by_day(year, data):
    # Convert 'date time' column to datetime format
    data['date time'] = pd.to_datetime(data['date time'])
    # Filter data for the desired year
    data_year = data[data['date time'].dt.year == year]
    grouped_data = data_year.groupby(data_year['date time'].dt.date).mean(numeric_only = True)#took mean values of all the data in that data
    # Plotting for each day
    plt.figure(figsize=(14, 5))
    plt.plot(grouped_data.index, grouped_data['close'], linestyle='-')
    plt.title(f'Closing Prices vs. Date for {year}')
    plt.xlabel('Date')
    plt.ylabel('Close Price')
    plt.show()
  #resource :chatgpt

plot_closing_day_by_day(2018,data)

"""# 1.3 candle stic plot"""

#candlestick chart
!pip install mplfinance
import mplfinance as fplt

print("MPLFinance Version : {}".format(fplt.__version__))

data['date time'] = pd.to_datetime(data['date time'])
data.set_index('date time', inplace=True)

start = '2005-01-03 09:30:00'
end= '2005-01-03 09:50:00'
subset_data = data.loc[start:end]
# Define the AddPlot object for volume
volume = fplt.make_addplot(subset_data["volume"], panel=1, color="red", width=0.7, ylabel="volume",
                          secondary_y=True)

# Plot the candlestick chart with volume on the secondary y-axis
fplt.plot(
    subset_data,
    type='candle',
    addplot=[volume],
    style='charles',
    title='stocks, jan - 2005 (with Volume)',
    ylabel='Price ($)',
)
####reference :https://coderzcolumn.com/tutorials/data-science/candlestick-chart-in-python-mplfinance-plotly-bokeh

"""# observations:
there are some outliers as we can see from the plot there are some unsual peaks in day by day and minute by minute plots  and there are no such missing values in the dataset
"""

#missing values
missing_values = data.isnull()
missing_count = missing_values.sum()

print(missing_count)

import numpy as np #utliers can represent extreme values that deviate significantly from the typical behavior of the dataset. reason for the peaks
from scipy.stats import zscore
z_scores = np.abs(zscore(data))
outliers = (z_scores > 3)
outlier_indices  = np.where(outliers)
print(data[outliers])

"""# Normalization methods"""

#normalization methods
from sklearn.preprocessing import MinMaxScaler,StandardScaler
data_for_normalization = data[['open','high','low','close','volume']]
minmax = MinMaxScaler()#minmax normalization
minmax_data = minmax.fit_transform(data_for_normalization)
standard = StandardScaler()#standard scalar
standard_data = standard.fit_transform(data_for_normalization)

"""In this dataset it  contains outliers, standard normalization (z-score normalization) is generally a better choice than min-max normalization. Here's why:

Standard normalization is more robust to outliers because it scales the data based on the mean and standard deviation. This means that extreme values have less influence on the scaled data.
"""

standard_data

"""a) **Trading Frequency** :

---


 i took long term frequency (multi week or multi year) this allows to track the trends in stock prices over years

b) **Buy-Ask Spread and Trade Commission:**


---


  im not  explicitly considered commissions in my dataset, assuming a moderate buy-ask spread and trade commission reflects a realistic trading environment. This accounts for the costs associated with trading.

c) **Single Stock Trading:**


---






 in this assignment i have  focused on collecting data for a single company This approach allows  to leverage your knowledge and expertise about that particular company.

# 4. LSTM Model
"""

import torch
import torch.nn as nn

class LSTM(nn.Module):
  def __init__(self,input_size,hidden_size,output_size,num_layers):
    super(LSTM,self).__init__()
    self.hidden_size = hidden_size
    self.num_layers = num_layers
    self.lstm = nn.LSTM(input_size,hidden_size,num_layers,batch_first = True)#lstm layer with input,hidden,num_layers dimensions
    self.fc = nn.Linear(hidden_size,output_size)#fully connected layer
  def forward(self, x):#forward function for lstm
    out, _ = self.lstm(x) #pass input to lstm layer
    out = self.fc(out[:, -1, :]) # apply fully connected layer to get output
    return out

def resample(data, x = '1d', t1 = '09:15:00', t2 = '15:29:00'): #resampling the dataset by day wise
    try:
        data.drop(columns = ['symbol'], axis = 1, inplace = True)

    except:
        pass

        agg = { "open": "first", #aggmolaration of columns for a particular day
                "high": "max",
                "low": "min",
                "close": "last",
                "volume": "sum"
              }

    data= data.between_time(t1, t2)
    data = data.resample('{}'.format(x), origin='start').agg(agg).dropna()
    return data

resampled_data = resample(data, x = '1d', t1 = '09:15:00', t2 = '15:29:00')

resampled_data.columns

resampled_data.shape

"""#5. DataLoader"""

#dataloader for the data
from torch.utils.data import Dataset, DataLoader #importing datset and data;loader from torch
class Stocks(Dataset): #defining a clss called stocks
    def __init__(self,data, sequence_length):  #sequence length is nothing but of how many time steps before the input we are giving
        self.data = data
        self.sequence_length = sequence_length

    def __len__(self):
        return len(self.data) - self.sequence_length

    def __getitem__(self, idx):
        idx += self.sequence_length
        features = self.data.iloc[idx - self.sequence_length:idx, :] # taking all the columns as input
        target = self.data.iloc[idx, 3]  # 'close' is the target
        return torch.tensor(features.values,dtype=torch.float32), torch.tensor(target, dtype=torch.float32)
#reference:https://www.kaggle.com/code/taronzakaryan/predicting-stock-price-using-lstm-model-pytorch?scriptVersionId=59632795&cellId=15

sequence_length = 15
dataset = Stocks(resampled_data,sequence_length)#passing the resampled_data in to stocks dataset
dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)

resampled_data

"""# 6.Training"""

#normalizing the data
data_for_normalization = resampled_data[['open', 'high', 'low', 'close', 'volume']]
minmax = MinMaxScaler()
normalized_data = minmax.fit_transform(data_for_normalization)

# Create a DataFrame with the normalized data
normalized_df = pd.DataFrame(normalized_data, columns=data_for_normalization.columns, index=data_for_normalization.index)

# Define the training and testing data
train_data = normalized_df['2005':'2020']
test_data = normalized_df['2021':'2022']

# Define the sequence length (adjust as needed)
sequence_length = 10

# Create datasets and data loaders
train_dataset = Stocks(train_data, sequence_length)
test_dataset = Stocks(test_data, sequence_length)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# Define the LSTM model
input_size = 5 # close, high, low , volume , open columns
hidden_size = 64
output_size = 1  # Assuming 'close' is the target
num_layers = 2

model = LSTM(input_size, hidden_size, output_size, num_layers)

# Define loss function and optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 10  # Adjust as needed

for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for batch_features, batch_targets in train_loader:
        optimizer.zero_grad() #clears the accumulated gradients
        outputs = model(batch_features) #passing training inputs to the model to get outputs
        loss = criterion(outputs, batch_targets.unsqueeze(1))
        loss.backward() #backpropagation
        optimizer.step() #updating the weights
        total_loss += loss.item()

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader)}') #printing loss for every step

# Testing loop to calculate validation loss
model.eval()
test_loss = 0

with torch.no_grad():
    for batch_features, batch_targets in test_loader:
        outputs = model(batch_features)
        loss = criterion(outputs, batch_targets.unsqueeze(1))
        test_loss += loss.item()

print(f'Test Loss: {test_loss/len(test_loader)}')

# Initialize lists to store predicted and actual values
predicted_values = []
actual_values = []

# Set model to evaluation mode
model.eval()

# Iterate through the test data
with torch.no_grad():
    for batch_features, batch_targets in test_loader:
        outputs = model(batch_features)
        predicted_values.extend(outputs.squeeze(1).tolist())  # Convert to list and extend
        actual_values.extend(batch_targets.tolist())  # Convert to list and extend

# Convert the lists to numpy arrays for easier manipulation
y_p= np.array(predicted_values)
a_v= np.array(actual_values)

y_p

"""# 7.Trading module"""

def trading_module(initial_balance, predicted_values, actual_values):
    stock_quantity = 0
    balance = initial_balance

    for i in range(1, len(predicted_values)):
        if predicted_values[i] > predicted_values[i-1]:
            # Buy stocks
            no_of_stocks_to_buy = balance // actual_values[i-1]
            stock_quantity += no_of_stocks_to_buy
            balance -= no_of_stocks_to_buy * actual_values[i-1]
        elif predicted_values[i] < predicted_values[i-1]:
            # Sell stocks
            balance += stock_quantity * actual_values[i-1]
            stock_quantity = 0

    # If there are remaining stocks at the end, sell them
    balance += stock_quantity * actual_values[-1]
    stock_quantity = 0

    return balance

"""# Testing"""

#8th qn testing the trading module using last 2 years test data
initial_balance = 10000
predicted_values =y_p
actual_values =a_v

final_balance = trading_module(initial_balance, predicted_values, actual_values)
print(f"Final Balance: {final_balance}")

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 4))

# Plotting actual values
plt.plot(actual_values, label='Actual', color='blue')

# Plotting predicted values
plt.plot(predicted_values, label='Predicted', color='red')

# Adding legend
plt.legend()

# Adding title and labels
plt.title('Predicted vs Actual Stock Prices')
plt.xlabel('Time')
plt.ylabel('Price')

# Display the plot
plt.show()

"""**a) Does the price prediction error increase as you go further from the last time on which it was trained?**

yes ,the prediction errror increase as u go further from the last time on which it was trained,because the market fluctuations are very high and not predictiable as we go further from the last time it was trained , this is the common phenomena in time series forecasting


**b) Can you profitably trade with the bid-ask spread and commissions taken into account?**


Including commissions typically reduces the overall profitability of a trading strategy. This is because commissions represent transaction costs that traders incur every time they buy or sell an asset. These costs can add up over time and eat into the profits generated by the trading strategy.


**c) How does your profitability compare to a simple buy-and-hold strategy over long term (e.g.
one or two years)?**

so for long time like 2 years it not profitable to just simply hold and buy strategy



"""

def buy_and_hold_strategy(initial_balance, initial_predicted_price, final_actual_price):
    # Calculate the number of units bought
    num_units_bought = initial_balance / initial_predicted_price

    # Calculate the final value of the investment
    final_value = num_units_bought * final_actual_price

    return final_value

# Example usage:
initial_balance = 10000
initial_predicted_price = y_p[0]  # Assuming the first predicted price is used for buying
final_actual_price = a_v[-1]  # Assuming the last actual price is used for selling

buy_and_hold_profit = buy_and_hold_strategy(initial_balance, initial_predicted_price, final_actual_price)

print(f'Buy-and-Hold Profit: {buy_and_hold_profit}')

#resource:chatgpt

